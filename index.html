<!DOCTYPE html>
<html lang="en">

<head>
  <title>James Townsend</title>
  <meta charset="utf-8">
  <link rel="shortcut icon" type="image/ico" href="favicon.ico">
</head>

<img src="images/profile.jpeg"
     alt="Jamie at Portmeirion"
     width="320"
     height="288">

<h1>James (Jamie) Townsend</h1>
<p><strong>I am currently 'on the market' and looking for a research position
  in the Netherlands.</strong>

<p>I am a machine learning researcher. I recently completed a PhD, on
<a href="https://arxiv.org/abs/2104.10544">lossless compression with latent
variable models</a>, supervised by
<a href="https://web4.cs.ucl.ac.uk/staff/D.Barber/">Professor David Barber</a>
at the <a href="https://www.ucl.ac.uk/ai-centre/">UCL AI Centre</a> in London.
Most of my research to date has been on deep generative models and lossless
compression. I'm also interested in unsupervised learning more generally,
approximate inference, Monte Carlo methods, optimization and the design of
machine learning software systems.

<p>During the PhD I spent a lot of time working on the Python/NumPy automatic
differentiation software
<a href="https://github.com/hips/autograd">Autograd</a>. I interned under the
tutelage of <a href="https://people.csail.mit.edu/mattjj/">Matthew Johnson</a>
at Google Brain in San Francisco in Spring 2018, where I was fortunate enough
to work on <a href="https://github.com/google/jax">JAX</a> during the early
stages of the project.

<p>I use the name <em>James</em> on publications and official documents; for
everything else, I use <em>Jamie</em>.</p>

Github: <a href="https://github.com/j-towns/">@j-towns</a><br>
Twitter: <a href="https://twitter.com/_j_towns">@_j_towns</a><br>
Google Scholar: <a
  href="https://scholar.google.com/citations?user=fhYBZTcAAAAJ">
  James Townsend</a><br>
Email: <code>james.townsend@cs.ucl.ac.uk</code>

<h1>Publications and preprints</h1>
<ul>
  <li>
    Daniel Severo*, James Townsend*, Ashish Khisti, Alireza Makhzani, and Karen
    Ullrich,
    <em>Compressing Multisets with Large Alphabets</em>,
    arXiv preprint: <a href="https://arxiv.org/abs/2107.09202">
      https://arxiv.org/abs/2001.09186</a>. *Equal contribution.
  </li>

  <li>
    Yangjun Ruan, Karen Ullrich, Daniel Severo, James Townsend, Ashish Khisti,
    Arnaud Doucet, Alireza Makhzani, and Chris J. Maddison,
    <em>
      Improving Lossless Compression Rates via Monte Carlo Bits-Back
      Coding</em>,
    International Conference on Machine Learning (ICML),
    2021.
    ArXiv preprint:
    <a href=https://arxiv.org/abs/2102.11086>
      https://arxiv.org/abs/2102.11086</a>.
  </li>

  <li>
    James Townsend and Iain Murray,
    <a href="https://openreview.net/forum?id=HgMwatrI1I">
      Lossless Compression with State Space Models Using Bits Back
      Coding</a>,
    Neural Compression: From Information Theory to Applications -- Workshop @
    ICLR 2021.
  </li>

  <li>
    James Townsend,
    <em>Lossless Compression with Latent Variable Models</em>,
    PhD Thesis,
    2021.
    ArXiv preprint: <a href="https://arxiv.org/abs/2104.10544">
      https://arxiv.org/abs/2104.10544</a>.
  </li>

  <li>
    James Townsend, Thomas Bird, Julius Kunze, and David Barber,
    <a href="https://openreview.net/forum?id=r1lZgyBYwS">HiLLoC: Lossless
      Image Compression with Hierarchical Latent Variable Models</a>,
    International Conference on Learning Representations (ICLR),
    2020.
  </li>

  <li>
    James Townsend,
    <em>A Tutorial on the Range Variant of Asymmetric Numeral Systems</em>,
    arXiv preprint: <a href="https://arxiv.org/abs/2001.09186">
      https://arxiv.org/abs/2001.09186</a>.
  </li>

  <li>
    James Townsend, Thomas Bird, and David Barber,
    <a href="https://openreview.net/forum?id=ryE98iR5tm">
      Practical Lossless Compression with Latent Variables Using Bits Back
      Coding</a>,
    International Conference on Learning Representations (ICLR),
    2019.
  </li>


  <li>
    Jonathan So, James Townsend, and Benoit Gaujac,
    <a href="http://approximateinference.org/2018/accepted/SoEtAl2018.pdf">
      EP Structured Variational Autoencoders</a>,
    1st Symposium on Advances in Approximate Bayesian Inference,
    2018.
  </li>

  <li>
    James Townsend, Niklas Koep, and Sebastian Weichwald,
    <a href="https://jmlr.org/papers/v17/16-177.html">
      Pymanopt: A Python Toolbox for Optimization on Manifolds using
      Automatic Differentiation</a>,
    Journal of Machine Learning Research, vol. 17, no. 137, pp. 1â€“5,
    2016.
  </li>
</ul>
</html>
