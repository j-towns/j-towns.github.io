<!DOCTYPE html>
<html lang="en-GB">

<head>
  <title>James Townsend</title>
  <meta charset="utf-8">
  <link rel="shortcut icon" type="image/ico" href="favicon.ico">
  <link href="style.css" rel="stylesheet">
</head>

<div id="main">
<h1>James (Jamie) Townsend</h1>
<div id="profile">
<img src="images/profile.jpg" alt="Jamie" width="120">
</div>

<p>I am a machine learning researcher, based at the <a
  href="https://amlab.science.uva.nl/">Amsterdam Machine Learning Lab
  (AMLab)</a>. I completed my PhD, on <a
  href="https://arxiv.org/abs/2104.10544">lossless compression with latent
  variable models</a>, in 2020, supervised by <a
href="http://web4.cs.ucl.ac.uk/staff/D.Barber/">Professor David Barber</a> at
the <a href="https://www.ucl.ac.uk/ai-centre/">UCL AI Centre</a> in London.
Most of my research to date has been on deep generative models and lossless
compression. I'm also interested in unsupervised learning more generally,
approximate inference, Monte Carlo methods, optimization and the design of
machine learning software systems.

<p>During the PhD I spent a lot of time working on the Python/NumPy automatic
differentiation software
<a href="https://github.com/hips/autograd">Autograd</a>. I interned under the
tutelage of <a href="https://people.csail.mit.edu/mattjj/">Matthew Johnson</a>
at Google Brain in San Francisco in Spring 2018, where I was fortunate enough
to work on <a href="https://github.com/google/jax">JAX</a> during the early
stages of the project.

<p>I use the name <em>James</em> on publications and official documents; for
everything else, I use <em>Jamie</em>.</p>

Github: <a href="https://github.com/j-towns/">@j-towns</a><br>
Twitter: <a href="https://twitter.com/_j_towns">@_j_towns</a><br>
Google Scholar: <a
  href="https://scholar.google.com/citations?user=fhYBZTcAAAAJ">
  James Townsend</a><br>
Email: <code>j.h.n.townsend@uva.nl</code>

<h2>Publications and preprints</h2>
<ul>
  <li>
    Daniel Severo, James Townsend, Ashish Khisti, Alireza Makhzani,
    <em>Random Edge Coding: One-Shot Bits-Back Coding of Large Labeled
      Graphs</em>,
    International Conference on Machine Learning (ICML),
    2023.
    ArXiv preprint:
    <a href=https://arxiv.org/abs/2305.09705>
      https://arxiv.org/abs/2305.09705</a>.
  </li>

  <li>
    James Townsend and Jan-Willem van de Meent,
    <em>Verified Reversible Programming for Verified Lossless Compression</em>, 2022.
    Presented at the Languages for Inference (LAFI) workshop at POPL 2023.
    ArXiv preprint:
    <a href="https://arxiv.org/abs/2211.09676">https://arxiv.org/abs/2211.09676</a>.
    Video:
    <a href="https://youtu.be/w8st4mOajgs?t=5742">https://youtu.be/w8st4mOajgs?t=5742</a>.
  </li>

  <li>
    Daniel Severo*, James Townsend*, Ashish Khisti, Alireza Makhzani, and Karen
    Ullrich,
    <a
      href="https://github.com/dgms-and-applications/2021/raw/d03dcbf17ce23de912f44132974dee13e530d25e/your_dataset_is_a_multiset_and.pdf">
      Your Dataset is a Multiset and You Should Compress it Like One</a>, 2021.
    Awarded best paper at the Deep Generative Models and Downstream
    Applications Workshop.
    OpenReview: <a href="https://openreview.net/forum?id=vjrsNCu8Km">
      https://openreview.net/forum?id=vjrsNCu8Km</a>. *Equal contribution.
  </li>

  <li>
    Julius Kunze, James Townsend, and David Barber,
    <a href="https://opt-ml.org/papers/2021/paper31.pdf">Adaptive Optimization
      with Examplewise Gradients</a>, OPT2021: 13th Annual Workshop on
    Optimization for Machine Learning, 2021.
    ArXiv preprint: <a href="https://arxiv.org/abs/2112.00174">
      https://arxiv.org/abs/2112.00174</a>.
  </li>

  <li>
    Daniel Severo*, James Townsend*, Ashish Khisti, Alireza Makhzani, and Karen
    Ullrich,
    <a href="https://sigport.org/documents/compressing-multisets-large-alphabets">
      Compressing Multisets with Large Alphabets</a>, appearing at the Data
    Compression Conference (DCC), 2022.
    ArXiv preprint: <a href="https://arxiv.org/abs/2107.09202">
      https://arxiv.org/abs/2001.09186</a>. *Equal contribution.
  </li>

  <li>
    Yangjun Ruan, Karen Ullrich, Daniel Severo, James Townsend, Ashish Khisti,
    Arnaud Doucet, Alireza Makhzani, and Chris J. Maddison,
    <a href="https://proceedings.mlr.press/v139/ruan21a.html">
      Improving Lossless Compression Rates via Monte Carlo Bits-Back
      Coding</a>,
    International Conference on Machine Learning (ICML),
    2021.
    ArXiv preprint:
    <a href=https://arxiv.org/abs/2102.11086>
      https://arxiv.org/abs/2102.11086</a>.
  </li>

  <li>
    James Townsend and Iain Murray,
    <a href="https://openreview.net/forum?id=HgMwatrI1I">
      Lossless Compression with State Space Models Using Bits Back
      Coding</a>,
    Neural Compression: From Information Theory to Applications -- Workshop @
    ICLR 2021.
  </li>

  <li>
    James Townsend,
    <em>Lossless Compression with Latent Variable Models</em>,
    PhD Thesis,
    2021.
    ArXiv preprint: <a href="https://arxiv.org/abs/2104.10544">
      https://arxiv.org/abs/2104.10544</a>.
  </li>

  <li>
    James Townsend*, Thomas Bird*, Julius Kunze, and David Barber,
    <a href="https://openreview.net/forum?id=r1lZgyBYwS">HiLLoC: Lossless
      Image Compression with Hierarchical Latent Variable Models</a>,
    International Conference on Learning Representations (ICLR),
    2020. *Equal contribution.
  </li>

  <li>
    James Townsend,
    <em>A Tutorial on the Range Variant of Asymmetric Numeral Systems</em>,
    2020.
    ArXiv preprint: <a href="https://arxiv.org/abs/2001.09186">
      https://arxiv.org/abs/2001.09186</a>.
  </li>

  <li>
    James Townsend, Thomas Bird, and David Barber,
    <a href="https://openreview.net/forum?id=ryE98iR5tm">
      Practical Lossless Compression with Latent Variables Using Bits Back
      Coding</a>,
    International Conference on Learning Representations (ICLR),
    2019.
  </li>


  <li>
    Jonathan So, James Townsend, and Benoit Gaujac,
    <a href="http://approximateinference.org/2018/accepted/SoEtAl2018.pdf">
      EP Structured Variational Autoencoders</a>,
    1st Symposium on Advances in Approximate Bayesian Inference,
    2018.
  </li>

  <li>
    James Townsend, Niklas Koep, and Sebastian Weichwald,
    <a href="https://jmlr.org/papers/v17/16-177.html">
      Pymanopt: A Python Toolbox for Optimization on Manifolds using
      Automatic Differentiation</a>,
    Journal of Machine Learning Research, vol. 17, no. 137, pp. 1â€“5,
    2016.
  </li>
</ul>

<h2>Blog posts and draft papers</h2>
<ul>
  <li>
    <a href="papers/qr-derivative.pdf">Differentiating the qr decomposition</a>
    (2018).
  </li>

  <li>
    <a href="2017/06/12/A-new-trick.html">A new trick for calculating
      Jacobian vector products</a> (2016).
  </li>

  <li>
    <a href="papers/svd-derivative.pdf">Differentiating the singular value
      decomposition</a> (2016).
  </li>
</ul>
</div>

</html>
